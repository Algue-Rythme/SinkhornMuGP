{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rU37BVmof05",
        "outputId": "71a18a48-3e1b-48e1-ac52-33010bdbbf78"
      },
      "outputs": [],
      "source": [
        "!pip install jaxopt\n",
        "!pip install ott-jax\n",
        "!pip install gpjax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO7R6V6hSrkM"
      },
      "source": [
        "## GP Jax preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEK4uZVXSpJx"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from jax.scipy.sparse.linalg import gmres\n",
        "from jax.lax import custom_linear_solve\n",
        "from jax.numpy.linalg import slogdet\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNwaueawSyaT"
      },
      "outputs": [],
      "source": [
        "from jax.config import config\n",
        "# config.update(\"jax_enable_x64\", True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VykE82iwSzZ0"
      },
      "outputs": [],
      "source": [
        "import jax.random as jr\n",
        "import gpjax as gpx\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVdnHFcbm6Yz"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# sns.set_context(\"notebook\")\n",
        "sns.set_context(\"paper\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsFVkR2e15OF"
      },
      "source": [
        "## GP Sinkhorn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2RVVuBg8Je5"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "pad_size = 2\n",
        "\n",
        "def cloud_coordinates():\n",
        "  img_size = 28 - 2*pad_size\n",
        "  steps = jnp.linspace(-1, 1., num=img_size, endpoint=True)\n",
        "  x, y = jnp.meshgrid(steps, steps)\n",
        "  x = x.flatten()\n",
        "  y = y.flatten()\n",
        "  grid = jnp.stack([x, y]).T\n",
        "  return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk7ES6-N3WDe"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Optional\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from jaxopt.tree_util import tree_l2_norm\n",
        "\n",
        "from ott.geometry import pointcloud\n",
        "import ott.core.sinkhorn as sinkhorn\n",
        "\n",
        "from jaxopt import LBFGS\n",
        "\n",
        "\n",
        "def mu_cloud_embedding(cloud, mu, \n",
        "                       init_dual=(None, None),\n",
        "                       **kwargs):\n",
        "  init_dual_cloud, init_dual_mu = init_dual  # for warm start\n",
        "  weights = cloud  # unpack distribution\n",
        "  mu_cloud, mu_weight = mu  # unpack distribution\n",
        "\n",
        "  sinkhorn_epsilon = kwargs.pop('sinkhorn_epsilon')\n",
        "  cloud_coords = cloud_coordinates()\n",
        "\n",
        "  mu_w = jax.nn.softmax(mu_weight) if mu_weight is not None else None  # ensure it is a probability distribution\n",
        "  mu_c = mu_cloud - jnp.mean(mu_cloud, axis=0, keepdims=True)  # invariance by translation : recenter mu around its mean\n",
        "  scale = 1.0\n",
        "  mu_c = scale * jnp.tanh(mu_c)\n",
        "\n",
        "  # common geometry for all images\n",
        "  geom = pointcloud.PointCloud(cloud_coords, mu_c,\n",
        "                               epsilon=sinkhorn_epsilon)\n",
        "\n",
        "  def sinkhorn_single_cloud(cloud_weight,\n",
        "                            init_dual_cloud, init_dual_mu):\n",
        "    out = sinkhorn.sinkhorn(geom, cloud_weight, mu_w,\n",
        "                            init_dual_a=init_dual_cloud,\n",
        "                            init_dual_b=init_dual_mu,\n",
        "                            **kwargs)\n",
        "    return out\n",
        "\n",
        "  parallel_sinkhorn = jax.vmap(sinkhorn_single_cloud,\n",
        "                               in_axes=(0, 0, 0),\n",
        "                               out_axes=0)\n",
        "  \n",
        "  outs = parallel_sinkhorn(weights, init_dual_cloud, init_dual_mu)\n",
        "  init_dual = outs.f, outs.g  # for warm start\n",
        "  return outs.g, init_dual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEc0SA8K9lrZ"
      },
      "outputs": [],
      "source": [
        "def mean_cloud_embedding(cloud, mu_params, init_dual, **kwargs):\n",
        "  del init_dual  # unused\n",
        "  coordinates, weights = cloud\n",
        "  mu_cloud, _ = mu_params\n",
        "  mean_cloud = jnp.sum(coordinates * weights[:,:,jnp.newaxis], axis=1, keepdims=True)\n",
        "  pairwise_dist = jnp.sum((mean_cloud - mu_cloud[jnp.newaxis,:,:])**2, axis=-1)\n",
        "  return pairwise_dist, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uue5naA2-Irw"
      },
      "outputs": [],
      "source": [
        "def mu_uniform_ball(sample_train, key, mu_size, radius=0.5, with_weight=False):\n",
        "  coords = cloud_coordinates()\n",
        "  dim = coords.shape[-1]\n",
        "  key_theta, key_r = jax.random.split(key)\n",
        "  mu_cloud = jax.random.normal(key_theta, shape=(mu_size, dim))\n",
        "  norms = jnp.sqrt(jnp.sum(mu_cloud**2, axis=1, keepdims=True))\n",
        "  mu_cloud = mu_cloud / norms\n",
        "  radii = jax.random.uniform(key_r, shape=(mu_size, 1))\n",
        "  mu_cloud = mu_cloud * radius * radii\n",
        "  centroids = jnp.sum(sample_train[:,:,jnp.newaxis] * coords[jnp.newaxis,:,:], axis=1)\n",
        "  centroids_center = jnp.mean(centroids, axis=0, keepdims=True)\n",
        "  mu_cloud = mu_cloud + centroids_center  # OT is invariant by translation\n",
        "  mu_weight = None\n",
        "  if with_weight:\n",
        "    mu_weight = jnp.zeros(len(mu_cloud))  # before softmax\n",
        "  return mu_cloud, mu_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPzgUvKz13Yf"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXCVwqOF84Vi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.config.experimental.set_visible_devices([], 'GPU')\n",
        "\n",
        "def img_to_cloud(image):\n",
        "  img_size = 28\n",
        "  sliced_cropped = slice(pad_size, img_size-pad_size, None)\n",
        "  image   = image[sliced_cropped, sliced_cropped]\n",
        "  weights = image.flatten()\n",
        "  weights = weights / jnp.sum(weights)\n",
        "  return weights\n",
        "\n",
        "\n",
        "# 4,6 for mnist toy\n",
        "# 5,7 for sandals, sneakers\n",
        "# 0,5 for tee-shirt, sandals\n",
        "def process_mnist(seed, ds_size, digits=[4, 6]):  \n",
        "  train_mnist, (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  # train_mnist, (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "  del train_mnist  # unused\n",
        "  # select two classes\n",
        "  target_0 = y_test == digits[0]\n",
        "  target_1 = y_test == digits[1]\n",
        "  sample_0 = x_test[target_0]\n",
        "  sample_1 = x_test[target_1]\n",
        "  # build subset\n",
        "  sample = jnp.concatenate([sample_0, sample_1])\n",
        "  target = jnp.concatenate([jnp.zeros(len(sample_0)), jnp.ones(len(sample_1))])\n",
        "  target = target.reshape((-1, 1))\n",
        "  # shuffle data\n",
        "  key = jax.random.PRNGKey(seed)\n",
        "  indices = jax.random.permutation(key, len(sample))\n",
        "  sample = sample[indices]\n",
        "  target = target[indices]\n",
        "  # keep few points\n",
        "  sample = sample[:ds_size]\n",
        "  target = target[:ds_size]\n",
        "  # make a cloud\n",
        "  sample_cloud = jax.vmap(img_to_cloud, in_axes=0, out_axes=0)(sample)\n",
        "  return sample_cloud, target, sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcQItt7c2nzz"
      },
      "source": [
        "## Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkwqdN8tOIAD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pprint import PrettyPrinter\n",
        "import numpy as onp\n",
        "\n",
        "pp = PrettyPrinter(indent=4)\n",
        "\n",
        "def plot_mu_points(mu_points, ax):\n",
        "  mu_points = onp.array(mu_points)\n",
        "  T = mu_points.shape[0]\n",
        "  num_pts = mu_points.shape[1]\n",
        "  points = list(map(str, range(1, num_pts+1)))*T\n",
        "  timesteps = onp.array([(i//num_pts) for i in range(T * num_pts)])\n",
        "  mu_points = mu_points - jnp.mean(mu_points, axis=1, keepdims=True)  # invariant by translation\n",
        "  scale = 1.0\n",
        "  mu_points = scale * jnp.tanh(mu_points)\n",
        "  mu_points = mu_points.reshape((-1, 2))\n",
        "  x_coord = mu_points[:,0]\n",
        "  y_coord = mu_points[:,1]\n",
        "  df = pd.DataFrame({'x':x_coord, 'y':y_coord, 'points':points, 'timesteps':timesteps})\n",
        "  sns.lineplot(data=df, x='x', y='y', hue='points', markers='x', sort=False, marker='o', linestyle='-', ms=15., ax=ax)\n",
        "  ax.set_title('$\\mu$ coordinates')\n",
        "\n",
        "def plot_loss(losses, ax):\n",
        "  timesteps = onp.arange(len(losses))\n",
        "  sns.lineplot(x=timesteps, y=losses, ax=ax)\n",
        "  ax.set_xlabel('Timesteps')\n",
        "  ax.set_ylabel('Loss')\n",
        "  ax.set_title('Negative Log Marginal Likelihood')\n",
        "\n",
        "def plot_mu_weights(mu_weights, ax):\n",
        "  if mu_weights is None:\n",
        "    return None\n",
        "  mu_weights = onp.array(mu_weights)\n",
        "  T = mu_weights.shape[0]\n",
        "  num_pts = mu_weights.shape[1]\n",
        "  points = list(map(str, range(1, num_pts+1)))*T\n",
        "  timesteps = onp.array([(i//num_pts) for i in range(T * num_pts)])\n",
        "  mu_weights = jax.nn.softmax(mu_weights.reshape((T, num_pts)), axis=-1)\n",
        "  mu_weights = mu_weights.flatten()\n",
        "  df = pd.DataFrame({'timesteps':timesteps, 'points':points, '$\\mu$ weights':mu_weights})\n",
        "  sns.lineplot(data=df, x='timesteps', y='$\\mu$ weights', hue='points', markers='x', sort=False, marker='.', linestyle='-', ax=ax)\n",
        "  ax.set_xlabel('Timesteps')\n",
        "  ax.set_ylabel('$\\mu$ weights')\n",
        "  ax.set_title('$\\mu$ weights')\n",
        "  sns.move_legend(ax, \"lower left\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5y04c-eJA9J"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
        "\n",
        "\n",
        "def evaluate(cloud_embedding_fn, kernel_params, mu_params,\n",
        "             posterior, likelihood, constrainer, unconstrainer,\n",
        "             sample_train, sample_test, y_train, y_test, prefix=''):\n",
        "  X_train, _ = cloud_embedding_fn(sample_train, mu_params)\n",
        "  X_test, _ = cloud_embedding_fn(sample_test, mu_params)\n",
        "  D = gpx.Dataset(X=X_train, y=y_train)\n",
        "  posterior_fn = posterior(D, kernel_params)\n",
        "\n",
        "  latent_dist = posterior_fn(X_test)\n",
        "  predictive_dist = likelihood(latent_dist, kernel_params)\n",
        "  predictive_mean = predictive_dist.mean()\n",
        "  predictive_std = predictive_dist.stddev()\n",
        "\n",
        "  try:\n",
        "    predictive_mean = predictive_mean.flatten()\n",
        "    evs = explained_variance_score(y_test.flatten(), predictive_mean)\n",
        "    label_pred = (predictive_mean >= 0.5).astype(y_test.dtype)\n",
        "    acc = jnp.mean(label_pred == y_test.flatten())\n",
        "  except Exception as e:\n",
        "    evs = float('nan')\n",
        "    acc = float('nan')\n",
        "  \n",
        "  log_likelihood = posterior.marginal_log_likelihood(D, constrainer)(gpx.transform(kernel_params, unconstrainer))\n",
        "  msg = f\"[GPJAX] TrainSetSize={len(X_train)} {prefix}Acc={acc*100:.3f}% evs={evs:.5f} log-likelihood={log_likelihood:.3f}\"\n",
        "  print(msg)\n",
        "  return evs, acc, log_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EvUCuLDn7PZ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def prettify_mu_str(mu):\n",
        "  max_len = 12\n",
        "  mu_selected = mu[0][:max_len]\n",
        "  return \"[\" + \", \".join([f\"({mu_p[0]:.3f},{mu_p[1]:.3f})\" for mu_p in mu_selected]) + \"]\"\n",
        "\n",
        "\n",
        "def learn(opt, opt_update, loss_fn,\n",
        "          init_mu, cloud_embedding_fn,\n",
        "          posterior, likelihood, constrainer, unconstrainer,\n",
        "          sample_train, sample_test,\n",
        "          y_train, y_test,\n",
        "          verbose=False):\n",
        "  ## init GP params\n",
        "  parameter_state = gpx.initialise(posterior, key=None)\n",
        "  constrained_kernel_params, _, _, _ = parameter_state.unpack()\n",
        "  kernel_params = gpx.transform(constrained_kernel_params, unconstrainer)\n",
        "\n",
        "  if verbose:\n",
        "    print('Unconstrained params:', end='');\n",
        "    pp.pprint(constrained_kernel_params)\n",
        "    print('Constrained params:', end='');\n",
        "    pp.pprint(kernel_params)\n",
        "\n",
        "  ## init Mu Sinkhorn\n",
        "  mu_params = init_mu(sample_train)\n",
        "  print(\"μ: \" + prettify_mu_str(mu_params))\n",
        "\n",
        "  ## Parameters to be optimized by LBFGS\n",
        "  params = {'kernel_params':kernel_params, 'mu_params':mu_params}\n",
        "\n",
        "  ## precomputation for speed-up\n",
        "  init_dual = cloud_embedding_fn(sample_train, mu_params)[1]\n",
        "\n",
        "  opt_state = opt.init_state(params, init_dual=init_dual)\n",
        "  mu_hist = [params['mu_params'][0]], [params['mu_params'][1]]\n",
        "  losses = [float(loss_fn(params, init_dual)[0])]\n",
        "  log_rate = 1\n",
        "  pb = tqdm(range(opt.maxiter))\n",
        "  for step in range(opt.maxiter):\n",
        "    params, opt_state = opt_update(params, opt_state, init_dual)\n",
        "    init_dual = opt_state.aux\n",
        "    loss_val = opt_state.value\n",
        "    mu_hist[0].append(params['mu_params'][0])\n",
        "    mu_hist[1].append(params['mu_params'][1])\n",
        "    losses.append(float(loss_val))\n",
        "    if step % log_rate == 0:\n",
        "      pb.update(log_rate)\n",
        "      kernel_params = params['kernel_params']\n",
        "      mu_params = params['mu_params']\n",
        "      mu_str = prettify_mu_str(mu_params)\n",
        "      kernel_params = gpx.transform(kernel_params, constrainer)\n",
        "      train_metrics = evaluate(cloud_embedding_fn, kernel_params, mu_params,\n",
        "           posterior, likelihood, constrainer, unconstrainer,\n",
        "           sample_train, sample_train, y_train, y_train, prefix='Train')\n",
        "      pb.set_postfix({\"Objective\": f\"{loss_val: .2f}\",\n",
        "                      \"TrainAcc\" : train_metrics[1]*100,\n",
        "                      # \"Kernel\"   : f\"{params['kernel_params']}\",\n",
        "                      # \"μ\":mu_str\n",
        "                      })\n",
        "  pb.close()\n",
        "  print('')\n",
        "\n",
        "  kernel_params = params['kernel_params']\n",
        "  mu_params = params['mu_params']\n",
        "  kernel_params = gpx.transform(kernel_params, constrainer)\n",
        "  pp.pprint(kernel_params)\n",
        "\n",
        "  test_metrics = evaluate(cloud_embedding_fn, kernel_params, mu_params,\n",
        "           posterior, likelihood, constrainer, unconstrainer,\n",
        "           sample_train, sample_test, y_train, y_test, prefix='Test')\n",
        "\n",
        "  return kernel_params, mu_params, (mu_hist, losses, test_metrics, train_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFlt1CvgMI12"
      },
      "outputs": [],
      "source": [
        "from jaxopt import OptaxSolver\n",
        "import optax\n",
        "\n",
        "\n",
        "def run_experiment(mu_sizes, seeds, sample_train, sample_test, y_train, y_test, *,\n",
        "                   with_weight):\n",
        "  ncols = 3\n",
        "  f, axes = plt.subplots(nrows=len(mu_sizes), ncols=ncols)\n",
        "  axes = onp.array(axes).reshape((len(mu_sizes), ncols))\n",
        "\n",
        "  kernel = gpx.RBF()\n",
        "  prior = gpx.Prior(kernel=kernel)\n",
        "  likelihood = gpx.Bernoulli(num_datapoints=len(sample_train))\n",
        "  # likelihood = gpx.Gaussian(num_datapoints=len(sample_train))\n",
        "  posterior = prior * likelihood\n",
        "\n",
        "  parameter_state = gpx.initialise(posterior, key=None)\n",
        "  _, trainable, constrainer, unconstrainer = parameter_state.unpack()\n",
        "\n",
        "  kwargs = dict(\n",
        "      sinkhorn_epsilon               = 1e-1 ,\n",
        "      lse_mode                       = True ,\n",
        "      implicit_differentiation       = False,\n",
        "      implicit_solver_ridge_kernel   = 1e-2 ,  # promote zero sum solutions\n",
        "      implicit_solver_ridge_identity = 1e-2 ,  # regul for ill-posed problem\n",
        "  )\n",
        "\n",
        "  cloud_embedding_fn = partial(mu_cloud_embedding, **kwargs)\n",
        "\n",
        "  def loss_fn(params, init_dual):\n",
        "    kernel_params = params['kernel_params']\n",
        "    mu_params = params['mu_params']\n",
        "    X_train, init_dual = cloud_embedding_fn(sample_train, mu_params, init_dual)\n",
        "    kernel_params = gpx.parameters.trainable_params(kernel_params, trainable)\n",
        "    D = gpx.Dataset(X=X_train, y=y_train)\n",
        "    nll = posterior.marginal_log_likelihood(D, constrainer, negative=True)\n",
        "    return nll(kernel_params), init_dual\n",
        "\n",
        "  opt = LBFGS(fun=loss_fn, maxiter=120, tol=1e-3, maxls=20, has_aux=True)\n",
        "  # optax_opt = optax.adam(learning_rate=5e-2)\n",
        "  # opt = OptaxSolver(opt=optax_opt, fun=loss_fn, maxiter=100, has_aux=True)\n",
        "\n",
        "  @jax.jit\n",
        "  def opt_update(params, opt_state, init_dual):\n",
        "    return opt.update(params, opt_state, init_dual=init_dual)\n",
        "\n",
        "  df_stats = []\n",
        "\n",
        "  for mu_size, ax in zip(mu_sizes, axes):\n",
        "\n",
        "    print( \"##########################################\")\n",
        "    print(f\"########### |μ|={mu_size:9d} ################\")\n",
        "    print( \"##########################################\")\n",
        "\n",
        "    test_metrics_avg = []\n",
        "    for i, seed in enumerate(seeds):\n",
        "\n",
        "      key = jax.random.PRNGKey(seed)\n",
        "      init_mu = partial(mu_uniform_ball, key=key,\n",
        "                        mu_size=mu_size, with_weight=with_weight)\n",
        "\n",
        "      kernel_params, mu_params, metrics = learn(opt, opt_update, loss_fn,\n",
        "            init_mu, cloud_embedding_fn,\n",
        "            posterior, likelihood, constrainer, unconstrainer,\n",
        "            sample_train, sample_test,\n",
        "            y_train, y_test)\n",
        "      \n",
        "      mu_hist, losses, test_metrics, train_metrics = metrics\n",
        "      test_metrics_avg.append(test_metrics)\n",
        "\n",
        "      if i+1 == len(seeds):\n",
        "        plot_mu_points(mu_hist[0], ax[0])\n",
        "        ax[0].axis('equal')\n",
        "        plot_mu_weights(mu_hist[1], ax[1])\n",
        "        plot_loss(losses, ax[2])\n",
        "        \n",
        "    test_metrics_avg = onp.array(test_metrics_avg)\n",
        "    df = pd.DataFrame(data=test_metrics_avg, columns=['EVS', 'TestAcc', 'log_likelihood'])\n",
        "    df['mu_size'] = mu_size\n",
        "    df_stats.append(df)\n",
        "\n",
        "  df_stats = pd.concat(df_stats, axis=0)\n",
        "  return (kernel_params, mu_params), df_stats, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CgeS-Rd_UAU8",
        "outputId": "a9c78cc5-bc3e-4337-85f1-6b94721a5229"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mu_sizes = [4]  # [1, 2, 3, 4, 5, 7, 10, 15, 20]\n",
        "seeds = [113]  #, 11, 55, 79, 46, 98, 73, 22, 34, 76]\n",
        "train_size = 200\n",
        "test_size = 1000  # less stochastic.\n",
        "ds_size = train_size + test_size\n",
        "ds_seeds = [911]\n",
        "tests_accs = []\n",
        "for ds_seed in ds_seeds:\n",
        "  sample_ds, target_ds, sample_naked = process_mnist(seed=ds_seed, ds_size=ds_size)\n",
        "  sample_train, sample_test, y_train, y_test = train_test_split(sample_ds, target_ds, train_size=train_size, shuffle=True, random_state=89)\n",
        "  plt.rcParams[\"figure.figsize\"] = (24, 8*len(mu_sizes))\n",
        "  (kernel_params, mu_params), df_stats, metrics = run_experiment(mu_sizes, seeds, sample_train, sample_test, y_train, y_test, with_weight=True)\n",
        "  test_metric = metrics[1]\n",
        "  test_acc = test_metric[1]\n",
        "  tests_accs.append(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "Kz2_XDyeuTTI",
        "outputId": "a0f07384-48ea-4443-f45f-36e2770b3da3"
      },
      "outputs": [],
      "source": [
        "df_stats.groupby('mu_size').mean()\n",
        "# df_stats.to_csv('toy2d.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "SDibirDWuN3W",
        "outputId": "1e5090c2-b8b3-4856-b736-e72e2e79b463"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (16+8, 8*len(mu_sizes))\n",
        "sns.set(font_scale=2)\n",
        "ncols = 2\n",
        "f, axes = plt.subplots(nrows=1, ncols=ncols)\n",
        "axes = onp.array(axes).reshape((1, ncols))\n",
        "mu_hist, losses, test_metrics, train_metrics = metrics\n",
        "for ax in axes:\n",
        "  ax[0].imshow(sample_train[2].reshape((24, 24)), extent=onp.array([-1, 1, -1, 1]), cmap='plasma')\n",
        "  plot_mu_points(mu_hist[0], ax[0])\n",
        "  ax[0].axis('equal')\n",
        "  plot_mu_weights(mu_hist[1], ax[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuOernr8oOcm"
      },
      "outputs": [],
      "source": [
        "def save_mu(mu_hist, losses):\n",
        "  mu_points, mu_weights = mu_hist\n",
        "  mu_points = onp.array(mu_points)\n",
        "  mu_weights = onp.array(mu_weights)\n",
        "\n",
        "  T = mu_weights.shape[0]\n",
        "  num_pts = mu_weights.shape[1]\n",
        "\n",
        "  points = list(map(str, range(1, num_pts+1)))*T\n",
        "  timesteps = onp.array([(i//num_pts) for i in range(T * num_pts)])\n",
        "\n",
        "  mu_weights = jax.nn.softmax(mu_weights, axis=-1)\n",
        "  mu_weights = mu_weights.flatten()\n",
        "  \n",
        "  # invariant by translation\n",
        "  mu_points = mu_points - jnp.mean(mu_points, axis=1, keepdims=True)  \n",
        "  scale = 1.\n",
        "  mu_points = scale * jnp.tanh(mu_points)\n",
        "  mu_points = mu_points.reshape((-1, 2))\n",
        "  x_coord = mu_points[:,0]\n",
        "  y_coord = mu_points[:,1]\n",
        "\n",
        "  losses = onp.array([[v]*num_pts for v in losses]).flatten()\n",
        "\n",
        "  df = pd.DataFrame({'timesteps':timesteps, 'points':points,\n",
        "                     '$\\mu$ weights':mu_weights,\n",
        "                     'x':x_coord, 'y':y_coord,\n",
        "                     'losses':losses})\n",
        "  df.to_csv('toy_metric.csv')\n",
        "\n",
        "\n",
        "def save_metrics(metrics):\n",
        "  mu_hist, losses, test_metrics, train_metrics = metrics\n",
        "  save_mu(mu_hist, losses)\n",
        "  test_metrics = onp.array([test_metrics])\n",
        "  train_metrics = onp.array([train_metrics])\n",
        "  train_test_metrics = onp.concatenate([train_metrics, test_metrics])\n",
        "  df_metrics = pd.DataFrame(data=train_test_metrics, columns=['evs', 'rmse', 'mae', 'log_likelihood'])\n",
        "  df_metrics['name'] = ['train', 'test']\n",
        "  df_metrics.to_csv('toy_score.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnRBaDa_Vgt2",
        "outputId": "3e06e9c3-950e-4c50-a471-ed53cd618a5f"
      },
      "outputs": [],
      "source": [
        "kwargs = dict(\n",
        "      sinkhorn_epsilon               = 1e-1 ,\n",
        "      lse_mode                       = True ,\n",
        "      implicit_differentiation       = False,\n",
        "      implicit_solver_ridge_kernel   = 1e-2 ,  # promote zero sum solutions\n",
        "      implicit_solver_ridge_identity = 1e-2 ,  # regul for ill-posed problem\n",
        ")\n",
        "cloud_embedding_fn = partial(mu_cloud_embedding, **kwargs)\n",
        "kernel = gpx.RBF()\n",
        "prior = gpx.Prior(kernel=kernel)\n",
        "likelihood = gpx.Bernoulli(num_datapoints=len(sample_train))\n",
        "# likelihood = gpx.Gaussian(num_datapoints=len(sample_train))\n",
        "posterior = prior * likelihood\n",
        "parameter_state = gpx.initialise(posterior, key=None)\n",
        "_, trainable, constrainer, unconstrainer = parameter_state.unpack()\n",
        "evaluate(cloud_embedding_fn, kernel_params, mu_params,\n",
        "           posterior, likelihood, constrainer, unconstrainer,\n",
        "           sample_train, sample_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4YECd6A3EQx",
        "outputId": "0d6ab781-2bc1-419f-b6c6-16abde92fd42"
      },
      "outputs": [],
      "source": [
        "mu_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwQirz989W-o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
